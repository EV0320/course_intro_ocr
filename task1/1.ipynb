{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/silevichar/liza/course_intro_ocr/task1\n"
     ]
    }
   ],
   "source": [
    "cd course_intro_ocr/task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/silevichar/liza/course_intro_ocr/task1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from course_intro_ocr_t1==0.1.1) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from course_intro_ocr_t1==0.1.1) (3.8.2)\n",
      "Requirement already satisfied: shapely in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from course_intro_ocr_t1==0.1.1) (2.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from matplotlib->course_intro_ocr_t1==0.1.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/silevichar/miniconda3/envs/efdl/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->course_intro_ocr_t1==0.1.1) (1.16.0)\n",
      "Building wheels for collected packages: course_intro_ocr_t1\n",
      "  Building editable for course_intro_ocr_t1 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for course_intro_ocr_t1: filename=course_intro_ocr_t1-0.1.1-py3-none-any.whl size=1276 sha256=b91b87cc501f3f329455b9abfa763e2a2f0d3226b9c6b1999f17f1f5f1a0668e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ymobz5ln/wheels/37/ad/4b/a41532ed0e2a03fe09e38e769dbe639574c2ad527611b043c4\n",
      "Successfully built course_intro_ocr_t1\n",
      "Installing collected packages: course_intro_ocr_t1\n",
      "  Attempting uninstall: course_intro_ocr_t1\n",
      "    Found existing installation: course_intro_ocr_t1 0.1.1\n",
      "    Uninstalling course_intro_ocr_t1-0.1.1:\n",
      "      Successfully uninstalled course_intro_ocr_t1-0.1.1\n",
      "Successfully installed course_intro_ocr_t1-0.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from course_intro_ocr_t1.data import MidvPackage\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from  torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_number = 4\n",
    "device = torch.device(device=f'cuda:{gpu_number}')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/silevichar/liza/midv500_compressed')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = Path().absolute().parent.parent / 'midv500_compressed'\n",
    "assert DATASET_PATH.exists(), DATASET_PATH.absolute()\n",
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, course_intro_ocr_t1.data.MidvPackage)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Собираем список пакетов (MidvPackage) \n",
    "data_packs = MidvPackage.read_midv500_dataset(DATASET_PATH)\n",
    "len(data_packs), type(data_packs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crop_dataset(Dataset):\n",
    "    def __init__(self, data_packs, train=True):\n",
    "        self.data_packs = data_packs\n",
    "        self.indices = []\n",
    "        self.device = device\n",
    "\n",
    "        for i, data_pack in enumerate(data_packs):\n",
    "            for j in range(len(data_pack)):\n",
    "                if train:\n",
    "                    if not data_pack[j].is_test_split():\n",
    "                        self.indices.append((i, j))\n",
    "                else:\n",
    "                    if data_pack[j].is_test_split():\n",
    "                        self.indices.append((i, j))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.indices[idx]\n",
    "        dp = self.data_packs[i][j]\n",
    "        image = np.array(dp.image.convert('RGB')) / 255.\n",
    "        mask = cv2.fillConvexPoly(np.zeros(image.shape[:2]), \n",
    "                                  np.array(dp.gt_data['quad']), (1,))[np.newaxis, ...]\n",
    "        \n",
    "        return torch.tensor(image.transpose(2, 0, 1), \n",
    "                            dtype=torch.float, \n",
    "                            device=device),\\\n",
    "                torch.tensor(mask, \n",
    "                             dtype=torch.float, \n",
    "                             device=device)\n",
    "\n",
    "    def get_key(self, idx):\n",
    "        i, j = self.indices[idx]\n",
    "        dp = self.data_packs[i][j]\n",
    "        return dp.unique_key\n",
    "    \n",
    "class EncConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.dec_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, e):\n",
    "        u = self.upsample(u)\n",
    "        pad_w = e.shape[2] - u.shape[2]\n",
    "        pad_h = e.shape[3] - u.shape[3]\n",
    "        padding = [pad_h // 2, pad_h - pad_h // 2, pad_w // 2, pad_w - pad_w // 2]\n",
    "        u = nn.functional.pad(u, padding)\n",
    "        return self.dec_conv(torch.cat((e, u), dim=1))\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_conv0 = EncConvBlock(3, 64)\n",
    "        self.pool0 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc_conv1 = EncConvBlock(64, 128)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc_conv2 = EncConvBlock(128, 256)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.enc_conv3 = EncConvBlock(256, 512)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck_conv = EncConvBlock(512, 1024)\n",
    "        self.up_0 = UpsampleBlock(1536, 512)\n",
    "        self.up_1 = UpsampleBlock(768, 256)\n",
    "        self.up_2 = UpsampleBlock(384, 128)\n",
    "        self.up_3 = UpsampleBlock(192, 64)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e0 = self.enc_conv0(x)\n",
    "        e1 = self.enc_conv1(self.pool0(e0))\n",
    "        e2 = self.enc_conv2(self.pool1(e1))\n",
    "        e3 = self.enc_conv3(self.pool2(e2))\n",
    "        b = self.bottleneck_conv(self.pool3(e3))\n",
    "        u0 = self.up_0(b, e3)\n",
    "        u1 = self.up_1(u0, e2)\n",
    "        u2 = self.up_2(u1, e1)\n",
    "        u3 = self.up_3(u2, e0)\n",
    "        out = self.out(u3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, opt, loss_fn, epochs, data_tr, data_val):\n",
    "    X_val, Y_val = next(iter(data_val))\n",
    "    train_loss, val_loss = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        tic = time()\n",
    "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
    "\n",
    "        avg_loss = 0\n",
    "        model.train()\n",
    "        for X_batch, Y_batch in tqdm(data_tr):\n",
    "\n",
    "            opt.zero_grad()\n",
    "            Y_pred = model(X_batch)\n",
    "            Y_pred = torch.sigmoid(Y_pred)\n",
    "            loss = loss_fn(Y_pred, Y_batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            avg_loss += loss / len(data_tr)\n",
    "        toc = time()\n",
    "        print('loss: %f' % avg_loss)\n",
    "        train_loss.append(avg_loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        Y_hat = model(X_val.to(device)).detach().cpu()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss_sum = 0\n",
    "            for X_val_batch, Y_val_batch in tqdm(data_val):\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    Y_pred_batch = model(X_val_batch)\n",
    "                    loss = loss_fn(torch.sigmoid(Y_pred_batch), Y_val_batch)\n",
    "                    prediction = torch.sigmoid(Y_pred_batch) > 0.5\n",
    "                val_loss_sum += loss\n",
    "            val_loss.append((val_loss_sum/len(data_val)).item())\n",
    "        \n",
    "        torch.save(model.state_dict(), f'epoch{epoch}.pth')\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = crop_dataset(data_packs)\n",
    "val_data = crop_dataset(data_packs, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = UNet()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('epoch5.pth'))\n",
    "\n",
    "train(model, torch.optim.Adam(model.parameters()), nn.BCELoss(), 10, train_loader, val_loader)\n",
    "\n",
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from course_intro_ocr_t1.metrics import dump_results_dict, measure_crop_accuracy\n",
    "\n",
    "results_dict = dict()\n",
    "\n",
    "def get_vertices(prediction):\n",
    "    mask = torch.sigmoid(prediction.cpu().detach()) > 0.5\n",
    "    nonzero = mask.nonzero()\n",
    "    sums = nonzero.sum(axis=1)\n",
    "    try:\n",
    "        top_left = nonzero[sums.argmin()]\n",
    "        bottom_right = nonzero[sums.argmax()]\n",
    "        diffs = nonzero[:, 0] - nonzero[:, 1]\n",
    "        top_right = nonzero[diffs.argmin()]\n",
    "        bottom_left = nonzero[diffs.argmax()]\n",
    "        return torch.stack([top_left, top_right, bottom_right, bottom_left]).numpy().astype(float)\n",
    "    except IndexError:\n",
    "        return torch.zeros((4, 2)).numpy().astype(float)\n",
    "\n",
    "for idx in tqdm(range(len(val_data))):\n",
    "    image, _ = val_data[idx]\n",
    "    result = model(image.unsqueeze(0))[0][0]\n",
    "    vertices = get_vertices(result)\n",
    "    vertices[:, 0] /= result.shape[0]\n",
    "    vertices[:, 1] /= result.shape[1]\n",
    "    vertices[:, [0, 1]] = vertices[:, [1, 0]]\n",
    "    key = val_data.get_key(idx)\n",
    "    results_dict[key] = vertices\n",
    "\n",
    "dump_results_dict(results_dict, Path() / 'pred.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
